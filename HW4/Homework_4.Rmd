---
title: "SDS 315 Homework 4"
author: "Brian Hu (bth997)"
date: "2/17/2025"

output: 
  html_document: 
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
options(scipen = 999)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(mosaic)
```

Link to Github repository with Rmd file: https://github.com/brianbevo/SDS_315/upload/main/HW4

## Question 1 

```{r}
setwd('/Users/brianhu/Desktop/SDS_315')


bank_sim <- rbinom(n = 100000, size = 2021, prob = 0.024)
pval_1 <- mean(bank_sim >= 70)
cat("P-value: ", pval_1)
hist(bank_sim, main = "Distribution of Flagged Trades from Simulation ", xlab = "Number of Flagged Trades")

```

The null hypothesis for this program is that Iron Bank's flagged trades proportion is not different from the SEC's baseline of 2.4% in its other traders. The results from this task display that their were an observed 70 flagged trades out of 2021 transactions from Iron Bank in which a Monte Carlo simulation with 100,000 iterations was conducted to test this hypothesis. The resulting p-value was 0.00191 which means that less than 0.2% of simulations produced 70 or more flagged trades out of the total trades given - 20218. This extremely low p-value suggests that the observed rate of flagged trades at Iron Bank is highly unlikely to occur and after seeing this result we reject the null hypothesis. Rather, it can be said that the evidence strongly supports further investigation into Iron Bank for potential illegal insider trading.

## Question 2 

```{r}


health_sim <- rbinom(n = 100000, size = 50, prob = 0.03)
pval2 <- mean(health_sim >= 8)
cat("P-value: ", pval2)
hist(health_sim, main = "Distribution of Health Code Violations from Simulation", xlab = "Number of Health Code Violations")

```

The null hypothesis for this problem assumes that Gourmet Bites' health code violation rate is acceptable and matches the health department's 3% baseline.  The results from this task display that from the given statistic that out of 50 Gourmet Bites restaurants 8 were revealed with health code violations, the Monte Carlo simulation used with 100,000 iterations yielded a p-value of 0.00013, meaning fewer than 0.013% of simulations produced 8 or more health code violations out of 50 restaurants. This low p-value means that Gourmet Bites' observed health code violation rate is highly unlikely by chance,so as a result we must reject the null hypothesis. Taking this further the evidence strongly suggests that Gourmet Bites should examined for potential issues with their health standards as they might not meet the health departments baseline.

## Question 3

```{r}

obs_val <- c(85, 56, 59, 27, 13)
expct_val<- c(0.30, 0.25, 0.20, 0.15, 0.10)

chi_square_test <- chisq.test(obs_val, p = expct_val)
chi_square_test

```

In this problem, the chi-squared test was used to compare the judge's jury selection with the county's population. From this our the null hypothesis assuming that there is no difference between these distributions. After performing a chi-squared test we get a value of 12.426 and a p-value of 0.014  which is below our accepted significant amount of .05, meaning that we reject the null hypothesis. This suggests a significant difference between the observed jury and expected county proportions which might lead to systematic bias in jury selection. An alternative explanation towards why the proportion is different could be variable such as outdated demographic data and varying response rates to jury duty among different groups. With these unpredictable influences, further investigation including analyzing more trials or looking at exclusion rates across different demographic groups could be.


## Question 4 

### Part A

```{r}
setwd('/Users/brianhu/Desktop/SDS_315')

letter <- read_csv("letter_frequencies.csv")
brown <- readLines("brown_sentences.txt")

clean <- function(text, freq_table) {
  word <- str_replace_all(text, "[^A-Za-z]", "")
  word <- toupper(word)
  
  
  obs_val <- table(factor(strsplit(word, "")[[1]], levels = freq_table$Letter))
  total <- sum(obs_val)
  expct_val <- total * freq_table$Probability
  chi_test <- sum((obs_val - expct_val)^2 / expct_val)
  return(chi_test)
}

letter$Probability <- letter$Probability / sum(letter$Probability)
chi_val <- sapply(brown, clean, freq_table = letter)  
hist(chi_val, main = "Distribution of Chi-squared Values for sentences", xlab = "Chi-squared Statistic", breaks = 50,
     col = "orange",
     border = "white")

```

### Part B 

```{r}

text <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum's new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker's inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project's effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone's expectations."
)

sentence_chi <- sapply(text, clean, freq_table = letter)
p_val <- sapply(sentence_chi, function(x) { sum(chi_val > x) / length(chi_val) })

table <- data.frame(
  Sentence <- 1:length(sentences),
  Chi_Squared <- sentence_chi,
  P_value <- round(p_val, 3) )

table <- setNames(table, c("Sentence", "Chi-Squared", "P-value"))
head(table, n = 10)

```


The null hypothesis assumes that letter frequencies in a given sentence match those in a relative similar distribution and appearence of typical English text. The chi-squared value we seek measures the difference between observed and expected values. To answer the question, the sentence 6, "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland," is most likely to be LLM-generated. Its high chi-squared value of 96.45 is in correlation with an extremely low p-value of 0.009 which to our standards is well below the 0.05 significance level. This suggests the sentence's letter distribution is significantly different from typical 'normal' English and that because of this there is a high likelihood that the sentence was made by a learning machine.